# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

Week 1 Practical aspects of Deep Learning
  - Setting up your Machine Learning Application
    - Train / Dev / Test sets
    - Bias / Variance
    - Basic Recipe for Machine Learning
  - Regularizing your neural network    
    - Regularization
    - Why regularization reduces overfitting?
    - Dropout Regularization
    - Understanding Dropout
    - Other regularization methods
  - Setting up your optimization problem    
    - Normalizing inputs
    - Vanishing / Exploding gradients
    - Weight Initialization for Deep Networks
    - Numerical approximation of gradients
    - Gradient checking
    - Gradient Checking Implementation Notes

Week 2 Optimization algorithms
  - Optimization algorithms
    - Mini-batch gradient descent
    - Understanding mini-batch gradient descent
    - Exponentially weighted averages
    - Understanding exponentially weighted averages
    - Bias correction in exponentially weighted averages
    - Gradient descent with momentum
    - RMSprop
    - Adam optimization algorithm
    - Learning rate decay
    - The problem of local optima
  - Heroes of Deep Learning (Optional)
    - Yuanqing Lin interview  
    
Week 3 Hyperparameter tuning, Batch Normalization and Programming Frameworks
  - Hyperparameter tuning
    - Tuning process
    - Using an appropriate scale to pick hyperparameters
    - Hyperparameters tuning in practice: Pandas vs. Caviar
  - Batch Normalization
    - Normalizing activations in a network
    - Fitting Batch Norm into a neural network
    - Why does Batch Norm work?
    - Batch Norm at test time
  - Multi-class classification
    - Softmax Regression
    - Training a softmax classifier
  - Introduction to programming frameworks
    - Deep learning frameworks
    - TensorFlow1
