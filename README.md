# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

Course can be found in [Coursera](https://www.coursera.org/learn/deep-neural-network)

Quiz and answers are collected for quick search in my blog [SSQ](https://ssq.github.io/2017/08/28/Coursera%20Ng%20Deep%20Learning%20Specialization%20Notebook/#Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization)

- Week 1 Practical aspects of Deep Learning
  - Recall that different types of initializations lead to different results
  - Recognize the importance of initialization in complex neural networks.
  - Recognize the difference between train/dev/test sets
  - Diagnose the bias and variance issues in your model
  - Learn when and how to use regularization methods such as dropout or L2 regularization.
  - Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them
  - Use gradient checking to verify the correctness of your backpropagation implementation
  - [x] [Initialization](https://github.com/SSQ/Coursera-Ng-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization/tree/master/Week%201%20PA%201)
  - [x] [Regularization](https://github.com/SSQ/Coursera-Ng-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization/tree/master/Week%201%20PA%202)
  - [x] [Gradient Checking](https://github.com/SSQ/Coursera-Ng-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization/tree/master/Week%201%20PA%203)
- Week 2 Optimization algorithms
  - Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
  - Use random minibatches to accelerate the convergence and improve the optimization
  - Know the benefits of learning rate decay and apply it to your optimization
  - [x] [Optimization](https://github.com/SSQ/Coursera-Ng-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization/tree/master/Week%202%20PA%201)
- Week 3 Hyperparameter tuning, Batch Normalization and Programming Frameworks
  - Master the process of hyperparameter tuning
  - Master the process of batch Normalization
  - [x] [Tensorflow](https://github.com/SSQ/Coursera-Ng-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization/tree/master/Week%203%20PA%201)
